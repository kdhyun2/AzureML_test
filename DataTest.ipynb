{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ACC = pd.read_csv(\"admin092503_202009251622_ACCELEROMETER.txt\")\n",
    "GRV = pd.read_csv(\"admin092503_202009251622_GRAVITY.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRV_XYZ = GRV.iloc[:,-3:]\n",
    "GRV_XYZ.columns = [\"gx\",\"gy\",\"gz\"]\n",
    "ACC_XYZ = ACC.iloc[:,-3:]\n",
    "ACC_XYZ.columns = [\"ax\",\"ay\",\"az\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = pd.concat([GRV_XYZ, ACC_XYZ], axis=1)\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result[\"X_gap\"] = sum_df[\"gx\"] - sum_df[\"ax\"]\n",
    "result[\"Y_gap\"] = sum_df[\"gy\"] - sum_df[\"ay\"]\n",
    "result[\"Z_gap\"] = sum_df[\"gz\"] - sum_df[\"az\"]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRV_time = pd.DataFrame()\n",
    "GRV_time = GRV[\"Time\"]\n",
    "\n",
    "result = pd.concat([GRV_time, result], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"Gravity_ACC_gap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENSOR DATA TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sql\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine, event\n",
    "from pandas.core.algorithms import value_counts\n",
    "import pyodbc\n",
    "import time, datetime\n",
    "import numpy as np\n",
    "from azure.storage.blob.blockblobservice import BlockBlobService\n",
    "from tempfile import NamedTemporaryFile\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(filename,container_name):\n",
    "    blob_service = BlockBlobService(\n",
    "            account_name='tokmadatastorage',\n",
    "            account_key='')\n",
    "    # 파일을 담을 임시 디렉토리 생성\n",
    "    local_file = NamedTemporaryFile()\n",
    "    # 블롭에서 Stream으로 데이터를 받아 Local_file 임시 디렉토리에 저장\n",
    "    blob_service.get_blob_to_stream(container_name, filename, stream=local_file, \n",
    "    max_connections=2)\n",
    "    # 저장된 파일의 0번 Memory 지정\n",
    "    local_file.seek(0)\n",
    "\n",
    "    # 지정된 메모리 리턴 \n",
    "    return local_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_timestamp_df(result_df):\n",
    "    stamp_list = []\n",
    "    for i in result_df[\"SENSING_TIME\"]:\n",
    "        s_time = i[:19]\n",
    "        stamp = int(time.mktime(datetime.datetime.strptime(str(s_time),'%Y-%m-%d %H:%M:%S').timetuple()))-32400\n",
    "        stamp_list.append(stamp)\n",
    "    stamp_df = pd.DataFrame(stamp_list)\n",
    "    stamp_df.columns = [\"TIMESTAMP_VAL\"]\n",
    "    result_df = pd.concat([result_df, stamp_df], axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_HRM(container_name, blob_name):\n",
    "    data = str(get_file(blob_name,container_name).read())[2:-1]\n",
    "    data = data.split(\"\\\\n\")[1:]\n",
    "    u_name = blob_name.split(\"/\")[0]\n",
    "    result = [] \n",
    "    for index in data:\n",
    "        obj = index.split(\",\")\n",
    "        result.append(obj)\n",
    "\n",
    "    u_name = blob_name.split(\"/\")[0]\n",
    "    result_df = pd.DataFrame(result).dropna().sort_values(by=0).drop([1],axis=1)\n",
    "    result_df.columns = [\"SENSING_TIME\", \"SENSOR_VAL\"]\n",
    "    result_df.sort_values([\"SENSING_TIME\"], inplace=True)\n",
    "    result_df = gen_timestamp_df(result_df)\n",
    "    result_df[\"SENSOR_TYPE\"] = \"HRM\"\n",
    "    result_df[\"SENIOR_ID\"] = u_name\n",
    "    KST = pytz.timezone('Asia/Seoul')\n",
    "    result_df[\"CREATE_DATE\"] = str(datetime.datetime.now(tz=KST))[:19]\n",
    "    result_df[\"CREATE_USER\"] = \"AF\"\n",
    "    result_df = result_df[[\"SENIOR_ID\",\"SENSOR_TYPE\",\"TIMESTAMP_VAL\",\"SENSOR_VAL\",\"SENSING_TIME\",\"CREATE_DATE\",\"CREATE_USER\"]]\n",
    "                                  \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_step(container_name, blob_name):\n",
    "    data = str(get_file(blob_name,container_name).read())[2:-1]\n",
    "    data = data.split(\"\\\\n\")[1:]\n",
    "    u_name = blob_name.split(\"/\")[0]\n",
    "    result = [] \n",
    "    for index in data:\n",
    "        obj = index.split(\",\")\n",
    "        result.append(obj)\n",
    "\n",
    "    u_name = blob_name.split(\"/\")[0]\n",
    "    result_df = pd.DataFrame(result).dropna().sort_values(by=0).drop([1],axis=1)\n",
    "    result_df.columns = [\"SENSING_TIME\", \"SENSOR_VAL\"]\n",
    "    result_df = result_df.astype({\"SENSOR_VAL\":int})\n",
    "    result_df[\"SENSOR_VAL\"] = result_df[\"SENSOR_VAL\"].sum()\n",
    "    result_df[\"SENSING_TIME\"] = result_df[\"SENSING_TIME\"].max()\n",
    "    result_df = gen_timestamp_df(result_df)\n",
    "    result_df[\"SENSOR_TYPE\"] = \"StepCount\"\n",
    "    result_df[\"SENIOR_ID\"] = u_name\n",
    "    KST = pytz.timezone('Asia/Seoul')\n",
    "    result_df[\"CREATE_DATE\"] = str(datetime.datetime.now(tz=KST))[:19]\n",
    "    result_df[\"CREATE_USER\"] = \"AF\"\n",
    "    result_df = result_df[[\"SENIOR_ID\",\"SENSOR_TYPE\",\"TIMESTAMP_VAL\",\"SENSOR_VAL\",\"SENSING_TIME\",\"CREATE_DATE\",\"CREATE_USER\"]]\n",
    "    result_df = result_df.head(1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client-Request-ID=2ad68f04-5b27-11ec-80d6-000d3a8db9c6 Retry policy did not allow for a retry: Server-Timestamp=Sun, 12 Dec 2021 08:40:32 GMT, Server-Request-ID=823bfdb7-b01e-004a-6233-ef140a000000, HTTP status code=404, Exception=The specified blob does not exist. ErrorCode: BlobNotFound<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.RequestId:823bfdb7-b01e-004a-6233-ef140a000000Time:2021-12-12T08:40:32.8994866Z</Message></Error>.\n"
     ]
    },
    {
     "ename": "AzureMissingResourceHttpError",
     "evalue": "The specified blob does not exist. ErrorCode: BlobNotFound\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\nRequestId:823bfdb7-b01e-004a-6233-ef140a000000\nTime:2021-12-12T08:40:32.8994866Z</Message></Error>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAzureMissingResourceHttpError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-03be58343e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontainer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"w-sensordata\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mblob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"minisoft11/STEPCOUNTER_202112021830.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_json_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontainer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-62d19f35ab2c>\u001b[0m in \u001b[0;36mget_json_step\u001b[0;34m(container_name, blob_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_json_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontainer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mu_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-fef25ec0f45a>\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(filename, container_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlocal_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# 블롭에서 Stream으로 데이터를 받아 Local_file 임시 디렉토리에 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     blob_service.get_blob_to_stream(container_name, filename, stream=local_file, \n\u001b[0m\u001b[1;32m      9\u001b[0m     max_connections=2)\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 저장된 파일의 0번 Memory 지정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/azure/storage/blob/baseblobservice.py\u001b[0m in \u001b[0;36mget_blob_to_stream\u001b[0;34m(self, container_name, blob_name, stream, snapshot, start_range, end_range, validate_content, progress_callback, max_connections, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout, cpk)\u001b[0m\n\u001b[1;32m   2191\u001b[0m                 \u001b[0mdownload_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;31m# Mark the first progress chunk. If the blob is small or this is a single\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/azure/storage/blob/baseblobservice.py\u001b[0m in \u001b[0;36mget_blob_to_stream\u001b[0;34m(self, container_name, blob_name, stream, snapshot, start_range, end_range, validate_content, progress_callback, max_connections, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout, cpk)\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0moperation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_OperationContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2147\u001b[0;31m             blob = self._get_blob(container_name,\n\u001b[0m\u001b[1;32m   2148\u001b[0m                                   \u001b[0mblob_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m                                   \u001b[0msnapshot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/azure/storage/blob/baseblobservice.py\u001b[0m in \u001b[0;36m_get_blob\u001b[0;34m(self, container_name, blob_name, snapshot, start_range, end_range, validate_content, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout, cpk, _context)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             check_content_md5=validate_content)\n\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         return self._perform_request(request, _parse_blob,\n\u001b[0m\u001b[1;32m   1884\u001b[0m                                      [blob_name, snapshot, validate_content, self.require_encryption,\n\u001b[1;32m   1885\u001b[0m                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_encryption_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_resolver_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/azure/storage/common/storageclient.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, request, parser, parser_args, operation_context, expected_errors)\u001b[0m\n\u001b[1;32m    444\u001b[0m                                  \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                                  exception_str_in_one_line)\n\u001b[0;32m--> 446\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;31m# If this is a location locked operation and the location is not set,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/azure/storage/common/storageclient.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, request, parser, parser_args, operation_context, expected_errors)\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAzureException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0mretry_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mretry_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/azure/storage/common/storageclient.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, request, parser, parser_args, operation_context, expected_errors)\u001b[0m\n\u001b[1;32m    357\u001b[0m                         \u001b[0;31m# This exception will be caught by the general error handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                         \u001b[0;31m# and raised as an azure http exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                         _http_error_handler(\n\u001b[0m\u001b[1;32m    360\u001b[0m                             HTTPError(response.status, response.message, response.headers, response.body))\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/azure/storage/common/_error.py\u001b[0m in \u001b[0;36m_http_error_handler\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAzureMissingResourceHttpError\u001b[0m: The specified blob does not exist. ErrorCode: BlobNotFound\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\nRequestId:823bfdb7-b01e-004a-6233-ef140a000000\nTime:2021-12-12T08:40:32.8994866Z</Message></Error>"
     ]
    }
   ],
   "source": [
    "container_name = \"w-sensordata\"\n",
    "blob_name = \"minisoft11/STEPCOUNTER_202112021830.txt\"\n",
    "df =  get_json_step(container_name=container_name, blob_name=blob_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "975ee0b139c52daa24b2c5f37d2f4d30d1dc229f11d5200a6cf31e29d8cb028d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
